# Intro

> 分布式系统的核心是通过网络来协调，共同完成一致任务的一些计算机。

这门课主要是关于基础设施服务的，比如大型网站的存储、大数据运算（如MapReduce）、P2P共享。分布式计算之所以如此重要的原因是，许多重要的基础设施都是在它之上建立的，它们需要多台计算机或者说本质上需要多台物理隔离的计算机。

## 动机和挑战

*为什么需要分布式系统？*
- 更高的计算**性能**。大量的并行运算、CPU、内存、磁盘。
- **容错**（tolerate faults）via 复制（replication）。
- 物理设备的分布。
- 通过隔离实现安全。

*挑战：*
- 复杂的并发问题。
- 局部故障。
- 很难实现性能潜力。

## Labs
- Lab 1: 简单的 MapReduce。
- Lab 2: Raft 协议。
- Lab 3: 基于 Raft 的分布式 KV 服务。
- Lab 4: 基于 multi Raft 的 sharded KV。

## 抽象和实现

这门课主要讨论**基础架构**的三个部分：
- **存储（重点）**
- 通信
- 计算

一大目标：通过抽象对应用程序隐藏分布式系统的复杂性。

实现工具：
- RPC
- 线程
- 并发控制
  
## 主要议题

### 容错

在1000台服务器的集群中，总有些东西出错（机器故障、运行错误、运行缓慢）。我们想要对应用隐藏这些问题。
- 可用性 Availability：服务能掩盖错误（在出错时继续正常运行）（错误需要在一定范围内，太大也不行） 
- 可恢复性 Recoverability：如果出现了问题，服务会停止，在修复之后系统仍然可以正常运行。
  
实现工具：
- 非易失性存储：如，存checkpoint
- 复制 Replication：任何一个多副本系统中，都会有一个关键的问题，比如说，我们有两台服务器，它们本来应该是有着相同的系统状态，现在的关键问题在于，这两个副本总是会意外地偏离同步的状态，而不再互为副本。对于任何一种使用复制实现容错的系统，我们都面临这个问题。lab2和lab3都是通过管理多副本来实现容错的系统，你将会看到这里究竟有多复杂。

### 一致性

一致性就是用来定义操作行为的概念。如，`Get(k)`得到最近一次`Put(k,v)`的值。实现良好的行为是很难的！副本服务器很难保持一致。

虽然强一致性可以确保get获取的是最新的数据，但是实现这一点的代价非常高。几乎可以确定的是，分布式系统的各个组件需要做大量的通信，才能实现强一致性。如果你有多个副本，那么不管get还是put都需要询问每一个副本。

人们常常会使用弱一致性系统，你只需要更新最近的数据副本，并且只需要从最近的副本获取数据。所以，弱一致对于应用程序来说很有用，并且它可以用来获取高的性能。

### 性能

目标：可扩展的吞吐量
- Nx服务器 -> Nx总吞吐量 via 并行CPU，磁盘，网络
- 随着N的增长，扩展变得越来越困难：
    + 负载不均衡
    + 木桶原理
    + 有些东西不因N增长而加速：初始化，交互
  
### 权衡

容错、一致性和性能是敌人。

容错和一致性需要通信：
- 如，发送数据用于备份。
- 如，检查数据是否是最新的。
- 通信通常是缓慢的，且不可扩展。

许多设计只提供弱一致性，以提升速度。

## MapReduce

在TB级数据集上进行数小时的计算，例如构建搜索索引，排序或分析web结构。
用户只需定义Map和Reduce函数。MapReduce隐藏并管理分布式的各个方面。

词频统计的MapReduce job的抽象视图：
```
  Input1 -> Map -> a,1 b,1
  Input2 -> Map ->     b,1
  Input3 -> Map -> a,1     c,1
                    |   |   |
                    |   |   -> Reduce -> c,1
                    |   -----> Reduce -> b,2
                    ---------> Reduce -> a,2
```

1. 输入已经被分成 M 个文件。
2. 对每个输入文件进行一次 `Map()`，输出`<k2, v2>`集合的中间文件。（每个`Map()`调用是一个 “task”）
3. 当Map完成时，MR对每个`k2`收集所有的`v2`，将每个`<k2, v2>`传给Reduce函数。 
4. `Reduce()`最终输出`<k2,v3>`的集合。

词频统计的Map和Reduce伪代码：
```
Map(k, v)
    split v into words
    for each word w
      emit(w, "1")

Reduce(k, v_set)
    emit(len(v_set))
```

### 优缺点

MR扩展性不错：
- N 个worker可能会得到 Nx 吞吐量。
- `Map()`可以并行运行，因为彼此独立。`Reduce()`同理。
  
MR隐藏了很多细节：
- 将应用程序代码发给worker服务器。
- 跟踪哪些task已经完成。
- “shuffling”中间数据，从Map到Reduce。
- 均衡服务器间的负载。
- 错误恢复。
  
输入和输出文件都存储在GFS集群中，MR需要巨大的并行输入和输出吞吐量。
GFS将文件切分成64MB的chunks分布在许多不同的服务器上。
Map和Reduce需要并行读或写。
GFS还将每个文件备份到2至3台服务器上。

### 论文细节

在论文的Figure 1中，有一个Master服务器，它负责分发task给worker服务器，并且记录进度。
- Master将task分给worker直到所有Map完成。
- Map的输出（中间数据）保存在本地磁盘。
- 中间数据用 `hash() % R` 对进行切分，得到 R 份，对应 R 个Reduce。
- 所有Map完成后，Master分发Reduce tasks。
- 每个Reduce从所有的Map worker上拿中间数据。
- 每个Reduce task将输出独立写到GFS。

### 什么限制了性能

在2004年的论文中，MR受限于网络容量。经过网络传输的部分：
- Map从GFS读取输入
- Reduce从Map读中间数据，写输出到GFS

### 如何最小化网络使用

- 尽量在带有输入数据的GFS服务器上运行Map任务。
  + 所有计算机都作为GFS和MR的worker。
  + Map从本地读取输入数据。
- 中间数据只经过一次网络传输。

### 如何实现负载均衡

如果 N-1 服务器必须等待 1 个慢服务器完成，即浪费且缓慢。但有些任务可能比其他任务耗时更长。

解决方案：让task的数量远大于worker的数量。
- Master给完成之前task的worker分配新task。
- 没有任务大到占据完成时间(希望如此)。
- 速度快的worker比速度慢的worker完成的任务更多，且完成的时间差不多。
  
### MR容错

*假如一个worker在 MapReduce job中崩溃了，需要重新运行整个 job吗？*

MR只重新运行失败的Map和Reduce。因为Map和Reduce是纯确定性函数，只要输入和参数一样，输出也一样。

#### worker故障恢复的细节
- Map worker：
  + Master发现worker不再响应ping。
  + Master知道哪个Map task运行在该worker上。
  + 该Map task的中间数据丢失了，Master通知其他worker重新运行task。若所有的Reduce都拿到了中间数据，可以不需要重新运行。
- Reduce worker:
  + 已完成的task没问题，其输出存储在GFS上，并带有备份。
  + 未完成的task由Master分给其他worker。
  
#### 其他异常问题

*如果 Master给两个 worker分配了相同的 Map task会怎样？*

> 可能是Master错误地认为其中一个worker挂了，导致一个task被分配了两次。然后，Master只会向Reduce worker通知正常工作的那个worker。

*如果 Master给两个 worker分配了相同的 Reduce task会怎样？*

> 它们都将尝试在GFS上写入相同的输出文件！原子GFS重命名防止搞混。

*如果一个 worker （“掉队者”）及其慢怎么办？*

> 也许是硬件的问题。Master会复制最后一些tasks。

*如果一个 worker产生了错误的输出，由于硬件或软件的问题*

> 糟透了！MR 对CPU和软件假定 “停止运行故障（fail-stop）”。

*Master发生故障了怎么办？*

> ...


### 今天的MapReduce

- 影响深远（Hadoop、Spark等）
- Google可能已经不再使用MapReduce。
  + MR 被 Flume / FlumeJava 取代。
  + GFS 被 Colossus（没有好的描述）和 BigTable 取代。